{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_data = \"..\\data\\Heart_disease.csv\"\n",
    "df_disease = pd.read_csv(disease_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_cat</th>\n",
       "      <th>bp_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.93</td>\n",
       "      <td>obese (class I)</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>48.25</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.71</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>61.83</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>95.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.98</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>63.10</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>90.0</td>\n",
       "      <td>145</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.05</td>\n",
       "      <td>obese (class II)</td>\n",
       "      <td>Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>60.07</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>82.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.40</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   1  55.38       1     156    85.0    140     90            3     1      0   \n",
       "1   3  48.25       2     169    82.0    150    100            1     1      0   \n",
       "2  12  61.83       2     178    95.0    130     90            3     3      0   \n",
       "3  32  63.10       1     158    90.0    145     85            2     2      0   \n",
       "4  46  60.07       2     173    82.0    140     90            3     1      0   \n",
       "\n",
       "   alco  active  cardio    BMI           BMI_cat bp_category  \n",
       "0     0       1       1  34.93   obese (class I)     Stage 2  \n",
       "1     0       1       1  28.71        overweight     Stage 2  \n",
       "2     0       1       1  29.98        overweight     Stage 1  \n",
       "3     0       1       1  36.05  obese (class II)     Stage 1  \n",
       "4     0       0       1  27.40        overweight     Stage 2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19788 entries, 0 to 19787\n",
      "Data columns (total 16 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           19788 non-null  int64  \n",
      " 1   age          19788 non-null  float64\n",
      " 2   gender       19788 non-null  int64  \n",
      " 3   height       19788 non-null  int64  \n",
      " 4   weight       19788 non-null  float64\n",
      " 5   ap_hi        19788 non-null  int64  \n",
      " 6   ap_lo        19788 non-null  int64  \n",
      " 7   cholesterol  19788 non-null  int64  \n",
      " 8   gluc         19788 non-null  int64  \n",
      " 9   smoke        19788 non-null  int64  \n",
      " 10  alco         19788 non-null  int64  \n",
      " 11  active       19788 non-null  int64  \n",
      " 12  cardio       19788 non-null  int64  \n",
      " 13  BMI          19788 non-null  float64\n",
      " 14  BMI_cat      19788 non-null  object \n",
      " 15  bp_category  19788 non-null  object \n",
      "dtypes: float64(3), int64(11), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_disease.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>BMI_cat_normal range</th>\n",
       "      <th>BMI_cat_obese (class I)</th>\n",
       "      <th>BMI_cat_obese (class II)</th>\n",
       "      <th>BMI_cat_obese (class III)</th>\n",
       "      <th>BMI_cat_overweight</th>\n",
       "      <th>BMI_cat_underweight</th>\n",
       "      <th>bp_category_Elevated</th>\n",
       "      <th>bp_category_Healthy</th>\n",
       "      <th>bp_category_Stage 1</th>\n",
       "      <th>bp_category_Stage 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  cholesterol  gluc  smoke  alco  active  cardio  female  male  \\\n",
       "0   1  55.38            3     1      0     0       1       1       1     0   \n",
       "\n",
       "   BMI_cat_normal range  BMI_cat_obese (class I)  BMI_cat_obese (class II)  \\\n",
       "0                     0                        1                         0   \n",
       "\n",
       "   BMI_cat_obese (class III)  BMI_cat_overweight  BMI_cat_underweight  \\\n",
       "0                          0                   0                    0   \n",
       "\n",
       "   bp_category_Elevated  bp_category_Healthy  bp_category_Stage 1  \\\n",
       "0                     0                    0                    0   \n",
       "\n",
       "   bp_category_Stage 2  \n",
       "0                    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one = df_disease.drop(columns=[\"ap_hi\", \"ap_lo\", \"height\", \"weight\", \"BMI\"])\n",
    "df_one = pd.get_dummies(df_one, columns=[\"gender\",\"BMI_cat\", \"bp_category\"])\n",
    "df_one.rename(columns= {\"gender_1\" : \"female\", \"gender_2\" : \"male\"}, inplace=True)\n",
    "df_one.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  active  cardio  \\\n",
       "0   1  55.38    140     90            3     1      0     0       1       1   \n",
       "\n",
       "     BMI  female  male  \n",
       "0  34.93       1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_two = df_disease.drop(columns=[\"BMI_cat\", \"bp_category\", \"height\", \"weight\"])\n",
    "df_two = pd.get_dummies(df_two, columns=[\"gender\"])\n",
    "df_two.rename(columns= {\"gender_1\" : \"female\", \"gender_2\" : \"male\"}, inplace=True)\n",
    "df_two.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19788, 19), (19788,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating train test split and validation for the first data frame\n",
    "X,y = df_one.drop(\"cardio\", axis = \"columns\"), df_one[\"cardio\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train = (13257, 19)\n",
      " X test = (3266, 19)\n",
      " X val = (3265, 19)\n",
      " y train = (13257,)\n",
      " y test = (3266,)\n",
      " y val = (3265,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"X train = {X_train.shape}\\n X test = {X_test.shape}\\n X val = {X_val.shape}\\n y train = {y_train.shape}\\n y test = {y_test.shape}\\n y val = {y_val.shape}\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling and normalization\n",
    "\n",
    "# creating a standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# creating a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fitting and transforming the data using standard scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Fit and transforming the data using the minmax scaler\n",
    "X_train_norm = minmax_scaler.fit_transform(X_train)\n",
    "X_val_norm = minmax_scaler.transform(X_val)\n",
    "\n",
    "# TODO write a little more about the scalers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split the second datframe and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train = (13257, 12)\n",
      " X test = (3266, 12)\n",
      " X val = (3265, 12)\n",
      " y train = (13257,)\n",
      " y test = (3266,)\n",
      " y val = (3265,)\n"
     ]
    }
   ],
   "source": [
    "X2,y2 = df_two.drop(\"cardio\", axis = \"columns\"), df_two[\"cardio\"]\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.33, random_state=42) \n",
    "X_val_2, X_test_2, y_val_2, y_test_2 = train_test_split(X_test_2, y_test_2, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# creating a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "# Fitting and transforming the data using standard scaler\n",
    "X_train_2_scaled = scaler.fit_transform(X_train_2)\n",
    "X_val_2_scaled = scaler.transform(X_val_2)\n",
    "# Fit and transforming the data using the minmax scaler\n",
    "X_train_2_norm = minmax_scaler.fit_transform(X_train_2)\n",
    "X_val_2_norm = minmax_scaler.transform(X_val_2)\n",
    "\n",
    "print(f\"X train = {X_train_2.shape}\\n X test = {X_test_2.shape}\\n X val = {X_val_2.shape}\\n y train = {y_train_2.shape}\\n y test = {y_test_2.shape}\\n y val = {y_val_2.shape}\"  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression Data set 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.70664506        nan        nan        nan 0.70853072\n",
      " 0.7062683  0.70596656 0.7062683  0.7062683  0.7062683  0.7062683\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70785198        nan        nan        nan 0.70837986\n",
      " 0.70619289 0.70619289 0.70619289 0.70619289 0.70619289 0.70619289\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7080782         nan        nan        nan 0.707173\n",
      " 0.70626827 0.70596659 0.70626827 0.70626827 0.70626827 0.70626827\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70815356        nan        nan        nan 0.70755024\n",
      " 0.70626827 0.70604203 0.70626827 0.70626827 0.70626827 0.70626827\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70724867        nan        nan        nan 0.70694704\n",
      " 0.70657005 0.70611744 0.70657005 0.70657005 0.70657005 0.7064946\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70709787        nan        nan        nan 0.70664523\n",
      " 0.70641919 0.70626833 0.70641919 0.70641919 0.70641919 0.70641919\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70694701        nan        nan        nan 0.70634366\n",
      " 0.70634377 0.7064946  0.70634377 0.70634377 0.70634377 0.70634377\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70679621        nan        nan        nan 0.7061928\n",
      " 0.70649466 0.70664549 0.70649466 0.70649466 0.70649466 0.70649466\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70672082        nan        nan        nan 0.70656993\n",
      " 0.70664552 0.70664552 0.70664552 0.70657008 0.70664552 0.70664552\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70634369        nan        nan        nan 0.70664538\n",
      " 0.70687182 0.70657008 0.70687182 0.70679638 0.70687182 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70604197        nan        nan        nan 0.7064946\n",
      " 0.70687179 0.70672093 0.70687179 0.70687179 0.70687179 0.70687179\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7061928         nan        nan        nan 0.70619283\n",
      " 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70634372        nan        nan        nan 0.70626827\n",
      " 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7064946         nan        nan        nan 0.70641916\n",
      " 0.70679635 0.70679635 0.70679635 0.70679635 0.70679635 0.70679635\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70664546        nan        nan        nan 0.70649463\n",
      " 0.70672093 0.70679635 0.70679635 0.70679635 0.70679635 0.70679635\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70672093        nan        nan        nan 0.70664549\n",
      " 0.70679638 0.70679635 0.70672093 0.70672093 0.70672093 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70664552        nan        nan        nan 0.70664552\n",
      " 0.70679638 0.70664552 0.70679638 0.70672093 0.70679638 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70672096        nan        nan        nan 0.7065701\n",
      " 0.70679638 0.70664552 0.70679638 0.70679638 0.70679638 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70664555        nan        nan        nan 0.70664555\n",
      " 0.70679638 0.70672096 0.70679638 0.70679638 0.70672093 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7065701         nan        nan        nan 0.70664552\n",
      " 0.70679638 0.70672096 0.70679638 0.70679638 0.70679638 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.74004869        nan        nan        nan 0.73509849\n",
      " 0.7274861  0.72767204 0.7274861  0.7274861  0.7274861  0.7274861\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73922524        nan        nan        nan 0.733493\n",
      " 0.72730103 0.72793289 0.72730103 0.72730103 0.72730103 0.72730103\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73690895        nan        nan        nan 0.73113971\n",
      " 0.72748443 0.72759492 0.72748443 0.72748443 0.72748443 0.72748443\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73514131        nan        nan        nan 0.73109562\n",
      " 0.72748443 0.72750321 0.72748443 0.72748443 0.72748443 0.72748443\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73285597        nan        nan        nan 0.72967965\n",
      " 0.72780163 0.72754497 0.72780163 0.72780163 0.72780163 0.72775825\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7314868         nan        nan        nan 0.72914693\n",
      " 0.72765268 0.72764055 0.72765268 0.72765268 0.72765268 0.72765268\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73114545        nan        nan        nan 0.72857441\n",
      " 0.72761117 0.72789539 0.72761117 0.72761117 0.72761117 0.72761117\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73012226        nan        nan        nan 0.72786088\n",
      " 0.72768634 0.72804393 0.72768634 0.72768634 0.72768634 0.72768634\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72951769        nan        nan        nan 0.72819891\n",
      " 0.72789798 0.72804193 0.72789798 0.72785471 0.72789798 0.72797086\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72858453        nan        nan        nan 0.72810422\n",
      " 0.72809878 0.72792754 0.72809878 0.72805545 0.72809878 0.72805545\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72799099        nan        nan        nan 0.72810244\n",
      " 0.72809696 0.72801231 0.72809696 0.72809696 0.72809696 0.72809696\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72806722        nan        nan        nan 0.72792051\n",
      " 0.72809696 0.72809696 0.72809696 0.72809696 0.72809696 0.72809696\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72821951        nan        nan        nan 0.7278995\n",
      " 0.72809696 0.72809696 0.72809696 0.72809696 0.72809696 0.72809696\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72824199        nan        nan        nan 0.72791153\n",
      " 0.72805356 0.72805356 0.72805356 0.72805356 0.72805356 0.72805356\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72824686        nan        nan        nan 0.72789813\n",
      " 0.72793984 0.72805356 0.72805356 0.72805356 0.72805356 0.72805356\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72829719        nan        nan        nan 0.72790156\n",
      " 0.72798324 0.72805356 0.72793984 0.72793984 0.72793984 0.72798324\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72803665        nan        nan        nan 0.72797268\n",
      " 0.72798324 0.72789828 0.72798324 0.72793984 0.72798324 0.72798324\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72815274        nan        nan        nan 0.72793051\n",
      " 0.72798324 0.72789828 0.72798324 0.72798324 0.72798324 0.72798324\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72804661        nan        nan        nan 0.72797244\n",
      " 0.72798324 0.72794168 0.72798324 0.72798324 0.72793984 0.72798324\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.72792911        nan        nan        nan 0.72797074\n",
      " 0.72798324 0.72794168 0.72798324 0.72798324 0.72798324 0.72798324\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.66412595        nan        nan        nan 0.67934625\n",
      " 0.6879799  0.68666272 0.6879799  0.6879799  0.6879799  0.6879799\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.66924837        nan        nan        nan 0.68198008\n",
      " 0.68812631 0.68680913 0.68812631 0.68812631 0.68812631 0.68812631\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.67437003        nan        nan        nan 0.68300432\n",
      " 0.68798011 0.68680924 0.68798011 0.68798011 0.68798011 0.68798011\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.67802843        nan        nan        nan 0.68432161\n",
      " 0.68798011 0.68724837 0.68798011 0.68798011 0.68798011 0.68798011\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.67993115        nan        nan        nan 0.68549195\n",
      " 0.68827294 0.68739468 0.68827294 0.68827294 0.68827294 0.68812653\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68227247        nan        nan        nan 0.68563836\n",
      " 0.68812653 0.68768739 0.68812653 0.68812653 0.68812653 0.68812653\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68256509        nan        nan        nan 0.68593108\n",
      " 0.68798022 0.68783381 0.68798022 0.68798022 0.68798022 0.68798022\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68417466        nan        nan        nan 0.68695576\n",
      " 0.68827294 0.68798022 0.68827294 0.68827294 0.68827294 0.68827294\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68519955        nan        nan        nan 0.68739478\n",
      " 0.68827305 0.68798022 0.68827305 0.68812663 0.68827305 0.68812663\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68593151        nan        nan        nan 0.68783402\n",
      " 0.68856576 0.68798022 0.68856576 0.68841935 0.68856576 0.68841935\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68622401        nan        nan        nan 0.6873951\n",
      " 0.68856566 0.68827294 0.68856566 0.68856566 0.68856566 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68651673        nan        nan        nan 0.68680945\n",
      " 0.68856566 0.68856566 0.68856566 0.68856566 0.68856566 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68666304        nan        nan        nan 0.68710217\n",
      " 0.68856566 0.68856566 0.68856566 0.68856566 0.68856566 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68710217        nan        nan        nan 0.6875413\n",
      " 0.68841924 0.68841924 0.68841924 0.68841924 0.68841924 0.68841924\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.6875413         nan        nan        nan 0.68783402\n",
      " 0.68841924 0.68841924 0.68841924 0.68841924 0.68841924 0.68841924\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68768772        nan        nan        nan 0.68827305\n",
      " 0.68856566 0.68841924 0.68841924 0.68841924 0.68841924 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68798043        nan        nan        nan 0.68812674\n",
      " 0.68856566 0.68827294 0.68856566 0.68841924 0.68856566 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68798043        nan        nan        nan 0.68798043\n",
      " 0.68856566 0.68827294 0.68856566 0.68856566 0.68856566 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68798043        nan        nan        nan 0.68812674\n",
      " 0.68856566 0.68841935 0.68856566 0.68856566 0.68841924 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.68798033        nan        nan        nan 0.68812663\n",
      " 0.68856566 0.68841935 0.68856566 0.68856566 0.68856566 0.68856566\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.69996882        nan        nan        nan 0.70603293\n",
      " 0.70706995 0.70645769 0.70706995 0.70706995 0.70706995 0.70706995\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70243147        nan        nan        nan 0.70672839\n",
      " 0.70706079 0.70666315 0.70706079 0.70706079 0.70706079 0.70706079\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70417184        nan        nan        nan 0.70616492\n",
      " 0.70707501 0.70650402 0.70707501 0.70707501 0.70707501 0.70707501\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70533587        nan        nan        nan 0.70683718\n",
      " 0.70707501 0.70669326 0.70707501 0.70707501 0.70707501 0.70707501\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70530277        nan        nan        nan 0.70677791\n",
      " 0.70738014 0.70678971 0.70738014 0.70738014 0.70738014 0.70728169\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70590191        nan        nan        nan 0.70660553\n",
      " 0.70722847 0.70698749 0.70722847 0.70722847 0.70722847 0.70722847\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70588975        nan        nan        nan 0.70648944\n",
      " 0.70713125 0.70719055 0.70713125 0.70713125 0.70713125 0.70713125\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70627404        nan        nan        nan 0.70670673\n",
      " 0.70732568 0.70734066 0.70732568 0.70732568 0.70732568 0.70732568\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70654429        nan        nan        nan 0.70710647\n",
      " 0.70743354 0.70734304 0.70743354 0.70733419 0.70743354 0.70738743\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70650774        nan        nan        nan 0.70729853\n",
      " 0.7076833  0.70728896 0.7076833  0.70758483 0.7076833  0.70758483\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70638173        nan        nan        nan 0.70706481\n",
      " 0.70768196 0.70748553 0.70768196 0.70768196 0.70768196 0.70768196\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70657721        nan        nan        nan 0.70666675\n",
      " 0.70768196 0.70768196 0.70768196 0.70768196 0.70768196 0.70768196\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70672595        nan        nan        nan 0.70681136\n",
      " 0.70768196 0.70768196 0.70768196 0.70768196 0.70768196 0.70768196\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70697023        nan        nan        nan 0.70705162\n",
      " 0.70758341 0.70758341 0.70758341 0.70758341 0.70758341 0.70758341\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70721005        nan        nan        nan 0.70719402\n",
      " 0.70752986 0.70758341 0.70758341 0.70758341 0.70758341 0.70758341\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70730997        nan        nan        nan 0.7074309\n",
      " 0.70762841 0.70758341 0.70752986 0.70752986 0.70752986 0.70762841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70734638        nan        nan        nan 0.70738895\n",
      " 0.70762841 0.70743275 0.70762841 0.70752986 0.70762841 0.70762841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70739964        nan        nan        nan 0.70729404\n",
      " 0.70762841 0.70743275 0.70762841 0.70762841 0.70762841 0.70762841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7073473         nan        nan        nan 0.70738953\n",
      " 0.70762841 0.7075313  0.70762841 0.70762841 0.70752986 0.70762841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70729106        nan        nan        nan 0.70738833\n",
      " 0.70762841 0.7075313  0.70762841 0.70762841 0.70762841 0.70762841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.7085307175128996\n",
      "best params{'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Val accuracy 0.6934150076569678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "scoring = [\"accuracy\",\"precision\",\"recall\",\"f1\"]\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, scoring= scoring, refit=\"accuracy\", cv=5, verbose=0)\n",
    "log_reg_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(f\"Train accuracy {log_reg_ss_cv.best_score_}\")\n",
    "print(f\"best params{log_reg_ss_cv.best_params_}\")\n",
    "print(f\"Val accuracy {log_reg_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.69638658        nan        nan        nan 0.69834765\n",
      " 0.70310006 0.70264771 0.70310006 0.70310006 0.70317547 0.70310006\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.69797054        nan        nan        nan 0.70566402\n",
      " 0.70415603 0.7037036  0.70415603 0.70415603 0.70415603 0.70415603\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70242053        nan        nan        nan 0.70513614\n",
      " 0.70430689 0.70340186 0.70438234 0.70438234 0.70430689 0.70438234\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70272224        nan        nan        nan 0.70679569\n",
      " 0.70423128 0.70362816 0.70423128 0.70423128 0.70423128 0.70423128\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70377848        nan        nan        nan 0.70672042\n",
      " 0.70408045 0.70430689 0.70408045 0.70408045 0.70408045 0.70408045\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70551339        nan        nan        nan 0.70724835\n",
      " 0.70445778 0.70415598 0.70445778 0.70445778 0.70445778 0.70445778\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7072483         nan        nan        nan 0.70619215\n",
      " 0.70491047 0.70392968 0.70491047 0.70491047 0.70491047 0.70491047\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7058153         nan        nan        nan 0.70702205\n",
      " 0.70521221 0.70453319 0.70528765 0.70528765 0.70528765 0.70528765\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70513626        nan        nan        nan 0.70649406\n",
      " 0.70551387 0.70551393 0.70551387 0.70551387 0.70551387 0.70551387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70498543        nan        nan        nan 0.70664495\n",
      " 0.7055894  0.70536318 0.7055894  0.7055894  0.7055894  0.7055894\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70445769        nan        nan        nan 0.7055137\n",
      " 0.7052878  0.70483517 0.7052878  0.7052878  0.7052878  0.7052878\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70506121        nan        nan        nan 0.70566464\n",
      " 0.70581579 0.70513688 0.70581579 0.70581579 0.70581579 0.70581579\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70528763        nan        nan        nan 0.70566473\n",
      " 0.70611759 0.70581581 0.70604214 0.70604214 0.70611756 0.70611756\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70558946        nan        nan        nan 0.70543851\n",
      " 0.70619297 0.70604212 0.70619297 0.70619297 0.70626842 0.70619297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70528765        nan        nan        nan 0.70543857\n",
      " 0.70626839 0.70641927 0.70626839 0.70626839 0.70634383 0.70626839\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70551398        nan        nan        nan 0.70581573\n",
      " 0.70649472 0.70604206 0.70649472 0.70649472 0.70649472 0.70649472\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70566493        nan        nan        nan 0.70589114\n",
      " 0.70634386 0.70619294 0.70634386 0.70634386 0.70634386 0.70634386\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7059667         nan        nan        nan 0.70619289\n",
      " 0.70619297 0.70626839 0.70619297 0.70619297 0.70626842 0.70619297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70619297        nan        nan        nan 0.70641922\n",
      " 0.70626842 0.70634383 0.70626842 0.70626842 0.70626842 0.70626842\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70641922        nan        nan        nan 0.70672096\n",
      " 0.70641922 0.70619294 0.70641922 0.70641922 0.70641922 0.70641922\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.7072483533064446\n",
      "best params{'C': 0.03359818286283781, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Val accuracy 0.6934150076569678\n"
     ]
    }
   ],
   "source": [
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_norm, y_train)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, cv=5)\n",
    "log_reg_norm_cv = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"Train accuracy {log_reg_norm_cv.best_score_}\")\n",
    "print(f\"best params{log_reg_norm_cv.best_params_}\")\n",
    "print(f\"Val accuracy {log_reg_norm_cv.best_estimator_.score(X_val_norm, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7347056490820221\n",
      "best params{'C': 0.026366508987303583, 'penalty': 'l1', 'solver': 'saga'}\n",
      "best estimator 0.7231240428790199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.73266919        nan        nan        nan 0.73463035\n",
      " 0.73402697 0.73432858 0.73402697 0.73402697 0.73410239 0.73402697\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342331        nan        nan        nan 0.73395142\n",
      " 0.73395139 0.73410227 0.73395139 0.73387597 0.73395139 0.73395139\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73395139        nan        nan        nan 0.7338005\n",
      " 0.73395127 0.73372509 0.73395127 0.73395127 0.73395127 0.73395127\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342343        nan        nan        nan 0.73380042\n",
      " 0.7338758  0.7338758  0.7338758  0.7338758  0.7338758  0.7338758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334793        nan        nan        nan 0.73470565\n",
      " 0.73364947 0.73380033 0.73364947 0.73357403 0.73364947 0.73364947\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312174        nan        nan        nan 0.73470548\n",
      " 0.7336495  0.7336495  0.7336495  0.7336495  0.7336495  0.7336495\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73380053        nan        nan        nan 0.73387575\n",
      " 0.73357409 0.73357411 0.73357409 0.73357409 0.73357409 0.73357409\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364964        nan        nan        nan 0.73402669\n",
      " 0.73372497 0.73380042 0.73372497 0.73372497 0.73372497 0.73372497\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342328        nan        nan        nan 0.73334776\n",
      " 0.73387583 0.73357409 0.73387583 0.73387583 0.73380042 0.73387583\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357406        nan        nan        nan 0.73319684\n",
      " 0.73357409 0.73342326 0.73357409 0.73357409 0.73357409 0.73364953\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73319687        nan        nan        nan 0.73327231\n",
      " 0.73334787 0.7334987  0.73334787 0.73334787 0.73334787 0.73334787\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312148        nan        nan        nan 0.73304601\n",
      " 0.73327246 0.73327246 0.73327246 0.73327246 0.73327246 0.73327246\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312146        nan        nan        nan 0.73327237\n",
      " 0.73327248 0.73319701 0.73327248 0.73327248 0.73327248 0.73327248\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334778        nan        nan        nan 0.73334781\n",
      " 0.73319704 0.73312157 0.73319704 0.73319704 0.73319704 0.73319704\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334781        nan        nan        nan 0.7334987\n",
      " 0.73304615 0.73312157 0.7331216  0.7331216  0.73312157 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327237        nan        nan        nan 0.73357411\n",
      " 0.73312157 0.73312157 0.73312157 0.73312157 0.73312157 0.73312157\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342323        nan        nan        nan 0.73357411\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73312157\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349867        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327243        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327243        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, cv=5)\n",
    "log_reg2_ss_cv = grid_search.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "print(f\"best score {log_reg2_ss_cv.best_score_}\")\n",
    "print(f\"best params{log_reg2_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {log_reg2_ss_cv.best_estimator_.score(X_val_2_scaled, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.72324027        nan        nan        nan 0.73274467\n",
      " 0.72557841 0.71743197 0.72557841 0.72557841 0.72557841 0.72557841\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73131159        nan        nan        nan 0.7331218\n",
      " 0.72746427 0.71901622 0.72746427 0.72746427 0.72746427 0.72746427\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349898        nan        nan        nan 0.73380079\n",
      " 0.72957645 0.72263724 0.72957645 0.72957645 0.72950101 0.72957645\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357446        nan        nan        nan 0.73387597\n",
      " 0.73025535 0.72557895 0.7303308  0.7303308  0.73025535 0.7303308\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73478141        nan        nan        nan 0.73432855\n",
      " 0.73025541 0.72731378 0.73025541 0.73025541 0.73025541 0.73033085\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73432863        nan        nan        nan 0.73349867\n",
      " 0.73146224 0.72799248 0.73146224 0.73146224 0.73138683 0.73146224\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7331971         nan        nan        nan 0.73327248\n",
      " 0.73221662 0.72995369 0.73221662 0.73221662 0.73214118 0.73221662\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73266905        nan        nan        nan 0.73349878\n",
      " 0.73266922 0.73221676 0.73266922 0.73266922 0.73259378 0.73251836\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73266897        nan        nan        nan 0.73372509\n",
      " 0.73236725 0.73236745 0.73236725 0.73236725 0.73236725 0.73236725\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327226        nan        nan        nan 0.73395125\n",
      " 0.73312174 0.73266908 0.73312174 0.73312174 0.73312174 0.73304632\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73387569        nan        nan        nan 0.73447929\n",
      " 0.73447966 0.73342346 0.73447966 0.73447966 0.73432877 0.73447966\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73372483        nan        nan        nan 0.7341776\n",
      " 0.73425342 0.73349904 0.73425342 0.73425342 0.73417797 0.734178\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73417746        nan        nan        nan 0.73387589\n",
      " 0.73417806 0.73372537 0.73417806 0.73417806 0.7342535  0.73417806\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349859        nan        nan        nan 0.7336495\n",
      " 0.73380087 0.73327285 0.73387631 0.73387631 0.73380087 0.73387631\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364953        nan        nan        nan 0.73342326\n",
      " 0.73342368 0.73357451 0.73342368 0.73342368 0.73342368 0.73342368\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7338003         nan        nan        nan 0.73372492\n",
      " 0.73395159 0.73342348 0.73395159 0.73395159 0.73395159 0.73395159\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364944        nan        nan        nan 0.73364944\n",
      " 0.73387609 0.73425325 0.73380064 0.73380064 0.73380064 0.73380064\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327226        nan        nan        nan 0.73349859\n",
      " 0.73425316 0.7343286  0.73425316 0.73425316 0.73425316 0.73425316\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73372492        nan        nan        nan 0.73357406\n",
      " 0.7341023  0.73417772 0.7341023  0.7341023  0.7341023  0.7341023\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364947        nan        nan        nan 0.73364947\n",
      " 0.73402672 0.73425305 0.73402672 0.73410216 0.73402672 0.73402672\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.734781405235396\n",
      "best params{'C': 0.026366508987303583, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "best estimator 0.7200612557427258\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_2_norm, y_train_2)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, cv=5)\n",
    "log_reg2_norm_cv = grid_search.fit(X_train_2_norm, y_train_2)\n",
    "\n",
    "print(f\"best score {log_reg2_norm_cv.best_score_}\")\n",
    "print(f\"best params{log_reg2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {log_reg2_norm_cv.best_estimator_.score(X_val_2_norm, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have now tested Logistic regression. It had a reasonable result, however I will now try other models to see if i get a better result. The best performing model was that of the second data set with standard scaler. This had the best estimator of the four"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now test Random Forest to see what kind of results that will predict with the four different scenario's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6735320289506279\n",
      "best params{'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 200}\n",
      "best estimator 0.6569678407350689\n"
     ]
    }
   ],
   "source": [
    "# Importing random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "hyper_param = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5, verbose=0)\n",
    "rand_for_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"best score {rand_for_ss_cv.best_score_}\")\n",
    "print(f\"best params{rand_for_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6730794264721528\n",
      "best params{'criterion': 'gini', 'max_features': 'log2', 'n_estimators': 150}\n",
      "best estimator 0.6633996937212864\n"
     ]
    }
   ],
   "source": [
    "hyper_param = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5, verbose=0)\n",
    "rand_for_norm_cv = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"best score {rand_for_norm_cv.best_score_}\")\n",
    "print(f\"best params{rand_for_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for_norm_cv.best_estimator_.score(X_val_norm, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7275408181436982\n",
      "best params{'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 100}\n",
      "best estimator 0.7289433384379785\n"
     ]
    }
   ],
   "source": [
    "hyper_param = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5, verbose=0)\n",
    "rand_for2_ss_cv = grid_search.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "print(f\"best score {rand_for2_ss_cv.best_score_}\")\n",
    "print(f\"best params{rand_for2_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for2_ss_cv.best_estimator_.score(X_val_2_scaled, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7261078519560336\n",
      "best params{'criterion': 'gini', 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "best estimator 0.7298621745788668\n"
     ]
    }
   ],
   "source": [
    "hyper_param = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5, verbose=0)\n",
    "rand_for2_norm_cv = grid_search.fit(X_train_2_norm, y_train)\n",
    "\n",
    "print(f\"best score {rand_for2_norm_cv.best_score_}\")\n",
    "print(f\"best params{rand_for2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for2_norm_cv.best_estimator_.score(X_val_2_norm, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6997051967640203\n",
      "best params{'alpha': 0.001, 'binarize': 0.2, 'fit_prior': True}\n",
      "best estimator 0.6909647779479327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    'fit_prior': [True, False],\n",
    "    'binarize': np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid, cv=5)\n",
    "mnb_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"best score {mnb_ss_cv.best_score_}\")\n",
    "print(f\"best params{mnb_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6991782036204784\n",
      "best params{'alpha': 1.0, 'binarize': 0.7000000000000001, 'fit_prior': True}\n",
      "best estimator 0.6900459418070444\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    'fit_prior': [True, False],\n",
    "    'binarize': np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid, cv=5)\n",
    "mnb_norm_cv = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"best score {mnb_norm_cv.best_score_}\")\n",
    "print(f\"best params{mnb_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_norm_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7232408385691276\n",
      "best params{'alpha': 10.0, 'binarize': 0.0, 'fit_prior': True}\n",
      "best estimator 0.7075038284839203\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    'fit_prior': [True, False],\n",
    "    'binarize': np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid, cv=5)\n",
    "mnb_ss2_cv = grid_search.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "print(f\"best score {mnb_ss2_cv.best_score_}\")\n",
    "print(f\"best params{mnb_ss2_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_ss2_cv.best_estimator_.score(X_val_2_scaled, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7239194578101096\n",
      "best params{'alpha': 0.001, 'binarize': 0.5, 'fit_prior': True}\n",
      "best estimator 0.691271056661562\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    'fit_prior': [True, False],\n",
    "    'binarize': np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid, cv=5)\n",
    "mnb_ss2_norm_cv = grid_search.fit(X_train_2_norm, y_train)\n",
    "\n",
    "print(f\"best score {mnb_ss2_norm_cv.best_score_}\")\n",
    "print(f\"best params{mnb_ss2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_ss2_norm_cv.best_estimator_.score(X_val_2_scaled, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-NSZCLOcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
