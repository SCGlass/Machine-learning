{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_data = \"..\\data\\Heart_disease.csv\"\n",
    "df_disease = pd.read_csv(disease_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_cat</th>\n",
       "      <th>bp_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.93</td>\n",
       "      <td>obese (class I)</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>48.25</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.71</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>61.83</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>95.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.98</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>63.10</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>90.0</td>\n",
       "      <td>145</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.05</td>\n",
       "      <td>obese (class II)</td>\n",
       "      <td>Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>60.07</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>82.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.40</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   1  55.38       1     156    85.0    140     90            3     1      0   \n",
       "1   3  48.25       2     169    82.0    150    100            1     1      0   \n",
       "2  12  61.83       2     178    95.0    130     90            3     3      0   \n",
       "3  32  63.10       1     158    90.0    145     85            2     2      0   \n",
       "4  46  60.07       2     173    82.0    140     90            3     1      0   \n",
       "\n",
       "   alco  active  cardio    BMI           BMI_cat bp_category  \n",
       "0     0       1       1  34.93   obese (class I)     Stage 2  \n",
       "1     0       1       1  28.71        overweight     Stage 2  \n",
       "2     0       1       1  29.98        overweight     Stage 1  \n",
       "3     0       1       1  36.05  obese (class II)     Stage 1  \n",
       "4     0       0       1  27.40        overweight     Stage 2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19788 entries, 0 to 19787\n",
      "Data columns (total 16 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           19788 non-null  int64  \n",
      " 1   age          19788 non-null  float64\n",
      " 2   gender       19788 non-null  int64  \n",
      " 3   height       19788 non-null  int64  \n",
      " 4   weight       19788 non-null  float64\n",
      " 5   ap_hi        19788 non-null  int64  \n",
      " 6   ap_lo        19788 non-null  int64  \n",
      " 7   cholesterol  19788 non-null  int64  \n",
      " 8   gluc         19788 non-null  int64  \n",
      " 9   smoke        19788 non-null  int64  \n",
      " 10  alco         19788 non-null  int64  \n",
      " 11  active       19788 non-null  int64  \n",
      " 12  cardio       19788 non-null  int64  \n",
      " 13  BMI          19788 non-null  float64\n",
      " 14  BMI_cat      19788 non-null  object \n",
      " 15  bp_category  19788 non-null  object \n",
      "dtypes: float64(3), int64(11), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_disease.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>BMI_cat_normal range</th>\n",
       "      <th>BMI_cat_obese (class I)</th>\n",
       "      <th>BMI_cat_obese (class II)</th>\n",
       "      <th>BMI_cat_obese (class III)</th>\n",
       "      <th>BMI_cat_overweight</th>\n",
       "      <th>BMI_cat_underweight</th>\n",
       "      <th>bp_category_Elevated</th>\n",
       "      <th>bp_category_Healthy</th>\n",
       "      <th>bp_category_Stage 1</th>\n",
       "      <th>bp_category_Stage 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  cholesterol  gluc  smoke  alco  active  cardio  female  male  \\\n",
       "0   1  55.38            3     1      0     0       1       1       1     0   \n",
       "\n",
       "   BMI_cat_normal range  BMI_cat_obese (class I)  BMI_cat_obese (class II)  \\\n",
       "0                     0                        1                         0   \n",
       "\n",
       "   BMI_cat_obese (class III)  BMI_cat_overweight  BMI_cat_underweight  \\\n",
       "0                          0                   0                    0   \n",
       "\n",
       "   bp_category_Elevated  bp_category_Healthy  bp_category_Stage 1  \\\n",
       "0                     0                    0                    0   \n",
       "\n",
       "   bp_category_Stage 2  \n",
       "0                    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one = df_disease.drop(columns=[\"ap_hi\", \"ap_lo\", \"height\", \"weight\", \"BMI\"])\n",
    "df_one = pd.get_dummies(df_one, columns=[\"gender\",\"BMI_cat\", \"bp_category\"])\n",
    "df_one.rename(columns= {\"gender_1\" : \"female\", \"gender_2\" : \"male\"}, inplace=True)\n",
    "df_one.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  active  cardio  \\\n",
       "0   1  55.38    140     90            3     1      0     0       1       1   \n",
       "\n",
       "     BMI  female  male  \n",
       "0  34.93       1     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_two = df_disease.drop(columns=[\"BMI_cat\", \"bp_category\", \"height\", \"weight\"])\n",
    "df_two = pd.get_dummies(df_two, columns=[\"gender\"])\n",
    "df_two.rename(columns= {\"gender_1\" : \"female\", \"gender_2\" : \"male\"}, inplace=True)\n",
    "df_two.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19788, 19), (19788,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating train test split and validation for the first data frame\n",
    "X,y = df_one.drop(\"cardio\", axis = \"columns\"), df_one[\"cardio\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train = (13257, 19)\n",
      " X test = (3266, 19)\n",
      " X val = (3265, 19)\n",
      " y train = (13257,)\n",
      " y test = (3266,)\n",
      " y val = (3265,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"X train = {X_train.shape}\\n X test = {X_test.shape}\\n X val = {X_val.shape}\\n y train = {y_train.shape}\\n y test = {y_test.shape}\\n y val = {y_val.shape}\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling and normalization\n",
    "\n",
    "# creating a standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# creating a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fitting and transforming the data using standard scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Fit and transforming the data using the minmax scaler\n",
    "X_train_norm = minmax_scaler.fit_transform(X_train)\n",
    "X_val_norm = minmax_scaler.transform(X_val)\n",
    "\n",
    "# TODO write a little more about the scalers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split the second datframe and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train = (13257, 12)\n",
      " X test = (3266, 12)\n",
      " X val = (3265, 12)\n",
      " y train = (13257,)\n",
      " y test = (3266,)\n",
      " y val = (3265,)\n"
     ]
    }
   ],
   "source": [
    "X2,y2 = df_two.drop(\"cardio\", axis = \"columns\"), df_two[\"cardio\"]\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.33, random_state=42) \n",
    "X_val_2, X_test_2, y_val_2, y_test_2 = train_test_split(X_test_2, y_test_2, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# creating a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "# Fitting and transforming the data using standard scaler\n",
    "X_train_2_scaled = scaler.fit_transform(X_train_2)\n",
    "X_val_2_scaled = scaler.transform(X_val_2)\n",
    "# Fit and transforming the data using the minmax scaler\n",
    "X_train_2_norm = minmax_scaler.fit_transform(X_train_2)\n",
    "X_val_2_norm = minmax_scaler.transform(X_val_2)\n",
    "\n",
    "print(f\"X train = {X_train_2.shape}\\n X test = {X_test_2.shape}\\n X val = {X_val_2.shape}\\n y train = {y_train_2.shape}\\n y test = {y_test_2.shape}\\n y val = {y_val_2.shape}\"  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression Data set 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "scoring = [\"accuracy\",\"precision\",\"recall\",\"f1\"]\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, scoring= scoring, refit=\"accuracy\", cv=5, verbose=0)\n",
    "log_reg_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(f\"Train accuracy {log_reg_ss_cv.best_score_}\")\n",
    "print(f\"best params{log_reg_ss_cv.best_params_}\")\n",
    "print(f\"Val accuracy {log_reg_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.7072482964110984\n",
      "best params{'C': 0.04281332398719394, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Val accuracy 0.692802450229709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'none' (deprecated), 'elasticnet', 'l2', 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.69638658        nan        nan        nan 0.69834765\n",
      " 0.70310006 0.70264771 0.70310006 0.70310006 0.70310006 0.70317547\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.69797054        nan        nan        nan 0.70566402\n",
      " 0.70415603 0.7037036  0.70415603 0.70415603 0.70415603 0.70415603\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70234508        nan        nan        nan 0.70513614\n",
      " 0.70430689 0.70340186 0.70438234 0.70438234 0.70430689 0.70438234\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70272224        nan        nan        nan 0.70679569\n",
      " 0.70423128 0.70362816 0.70423128 0.70423128 0.70423128 0.70423128\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70385392        nan        nan        nan 0.70664498\n",
      " 0.70408045 0.70430689 0.70408045 0.70408045 0.70408045 0.70408045\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70551339        nan        nan        nan 0.70717291\n",
      " 0.70445778 0.70415598 0.70445778 0.70445778 0.70445778 0.70445778\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7072483         nan        nan        nan 0.70626759\n",
      " 0.70491047 0.70392968 0.70491047 0.70491047 0.70498588 0.70498588\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70589072        nan        nan        nan 0.70702205\n",
      " 0.70521221 0.70453319 0.70528765 0.70528765 0.70521221 0.70521221\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70513626        nan        nan        nan 0.70641865\n",
      " 0.70551387 0.70551393 0.70551387 0.70551387 0.70551387 0.70551387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70498543        nan        nan        nan 0.70672036\n",
      " 0.7055894  0.70536318 0.7055894  0.7055894  0.7055894  0.7055894\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70445769        nan        nan        nan 0.70558911\n",
      " 0.7052878  0.70483517 0.7052878  0.7052878  0.7052878  0.7052878\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70506121        nan        nan        nan 0.7055892\n",
      " 0.70581579 0.70513688 0.70581579 0.70581579 0.70581579 0.70596667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70521218        nan        nan        nan 0.70566473\n",
      " 0.70611759 0.70581581 0.70604214 0.70604214 0.7059667  0.70604214\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70558946        nan        nan        nan 0.70543851\n",
      " 0.70619297 0.70604212 0.70619297 0.70619297 0.70619297 0.70619297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70528765        nan        nan        nan 0.70536315\n",
      " 0.70626839 0.70641927 0.70626839 0.70626839 0.70641927 0.70626839\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70566487        nan        nan        nan 0.70581573\n",
      " 0.70649472 0.70604206 0.70649472 0.70649472 0.7064193  0.70649472\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70566493        nan        nan        nan 0.7058157\n",
      " 0.70634386 0.70619294 0.70634386 0.70634386 0.70634386 0.70634386\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7059667         nan        nan        nan 0.70619289\n",
      " 0.70619297 0.70626839 0.70619297 0.70619297 0.70619297 0.70619297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70619297        nan        nan        nan 0.70641922\n",
      " 0.70626842 0.70634383 0.70626842 0.70626842 0.70626842 0.70626842\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7063438         nan        nan        nan 0.70672096\n",
      " 0.70641922 0.70619294 0.70641922 0.70641922 0.70641922 0.70641922\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_norm, y_train)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, cv=5)\n",
    "log_reg_norm_cv = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"Train accuracy {log_reg_norm_cv.best_score_}\")\n",
    "print(f\"best params{log_reg_norm_cv.best_params_}\")\n",
    "print(f\"Val accuracy {log_reg_norm_cv.best_estimator_.score(X_val_norm, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7347056490820221\n",
      "best params{'C': 0.026366508987303583, 'penalty': 'l1', 'solver': 'saga'}\n",
      "best estimator 0.7231240428790199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'none' (deprecated), 'elasticnet', 'l2', 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.73259378        nan        nan        nan 0.73463035\n",
      " 0.73402697 0.73432858 0.73402697 0.73402697 0.73410239 0.73402697\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349876        nan        nan        nan 0.73395142\n",
      " 0.73395139 0.73410227 0.73395139 0.73387597 0.73395139 0.73395139\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73402683        nan        nan        nan 0.7338005\n",
      " 0.73395127 0.73372509 0.73395127 0.73395127 0.73395127 0.73395127\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342343        nan        nan        nan 0.73380042\n",
      " 0.7338758  0.7338758  0.7338758  0.7338758  0.73395125 0.7338758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334793        nan        nan        nan 0.73470565\n",
      " 0.73364947 0.73380033 0.73364947 0.73357403 0.73364947 0.73364947\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334804        nan        nan        nan 0.73470548\n",
      " 0.7336495  0.7336495  0.7336495  0.7336495  0.7336495  0.7336495\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73387597        nan        nan        nan 0.73387575\n",
      " 0.73357409 0.73357411 0.73357409 0.73357409 0.73357409 0.73357409\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7335742         nan        nan        nan 0.73402669\n",
      " 0.73372497 0.73380042 0.73372497 0.73372497 0.73372497 0.73372497\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342328        nan        nan        nan 0.73327231\n",
      " 0.73387583 0.73357409 0.73387583 0.73387583 0.73387583 0.73387583\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357406        nan        nan        nan 0.73319684\n",
      " 0.73357409 0.73342326 0.73357409 0.73357409 0.73357409 0.73357409\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73319687        nan        nan        nan 0.73319687\n",
      " 0.73334787 0.7334987  0.73334787 0.73334787 0.73334787 0.73334787\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312148        nan        nan        nan 0.7329706\n",
      " 0.73327246 0.73327246 0.73327246 0.73327246 0.73327246 0.73327246\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312146        nan        nan        nan 0.73327237\n",
      " 0.73327248 0.73319701 0.73327248 0.73327248 0.7331216  0.73327248\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334778        nan        nan        nan 0.73334781\n",
      " 0.73319704 0.73312157 0.73319704 0.73319704 0.73319704 0.73319704\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334781        nan        nan        nan 0.7334987\n",
      " 0.73304615 0.73312157 0.7331216  0.7331216  0.73319701 0.73304615\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327237        nan        nan        nan 0.73357411\n",
      " 0.73312157 0.73312157 0.73312157 0.73312157 0.73312157 0.73312157\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342323        nan        nan        nan 0.73357411\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349867        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334784        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327243        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, cv=5)\n",
    "log_reg2_ss_cv = grid_search.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "print(f\"best score {log_reg2_ss_cv.best_score_}\")\n",
    "print(f\"best params{log_reg2_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {log_reg2_ss_cv.best_estimator_.score(X_val_2_scaled, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'none' (deprecated), 'elasticnet', 'l2', 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.72324027        nan        nan        nan 0.73266922\n",
      " 0.72557841 0.71743197 0.72557841 0.72557841 0.72550297 0.72550297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73131159        nan        nan        nan 0.7331218\n",
      " 0.72746427 0.71901622 0.72746427 0.72746427 0.72746427 0.72746427\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349898        nan        nan        nan 0.73380079\n",
      " 0.72957645 0.72263724 0.72957645 0.72957645 0.72957645 0.72957645\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357446        nan        nan        nan 0.73387597\n",
      " 0.73025535 0.72557895 0.7303308  0.7303308  0.7303308  0.7303308\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73478141        nan        nan        nan 0.73440396\n",
      " 0.73025541 0.72731378 0.73025541 0.73025541 0.73033085 0.73025541\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73432863        nan        nan        nan 0.73349867\n",
      " 0.73146224 0.72799248 0.73146224 0.73146224 0.73153769 0.73153769\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7331971         nan        nan        nan 0.73327248\n",
      " 0.73221662 0.72995369 0.73221662 0.73221662 0.73221662 0.73221662\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73266905        nan        nan        nan 0.73349878\n",
      " 0.73266922 0.73221676 0.73266922 0.73266922 0.73259378 0.73259381\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73266897        nan        nan        nan 0.73372509\n",
      " 0.73236725 0.73236745 0.73236725 0.73236725 0.73244267 0.73236725\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327226        nan        nan        nan 0.73395125\n",
      " 0.73312174 0.73266908 0.73312174 0.73312174 0.73312174 0.73312174\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73387569        nan        nan        nan 0.7343284\n",
      " 0.73447966 0.73342346 0.73447966 0.73447966 0.73447966 0.73440422\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73372483        nan        nan        nan 0.73417763\n",
      " 0.73425342 0.73349904 0.73425342 0.73425342 0.73425342 0.73425342\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73417746        nan        nan        nan 0.73387589\n",
      " 0.73417806 0.73372537 0.73417806 0.73417806 0.7342535  0.73417806\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357403        nan        nan        nan 0.7336495\n",
      " 0.73380087 0.73327285 0.73387631 0.73387631 0.73380087 0.73387631\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364953        nan        nan        nan 0.73342326\n",
      " 0.73342368 0.73357451 0.73342368 0.73342368 0.73342368 0.73342368\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73387575        nan        nan        nan 0.73380033\n",
      " 0.73395159 0.73342348 0.73395159 0.73395159 0.73395159 0.73395159\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364944        nan        nan        nan 0.73357403\n",
      " 0.73387609 0.73425325 0.73380064 0.73380064 0.73380064 0.73380064\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7333477         nan        nan        nan 0.73349859\n",
      " 0.73425316 0.7343286  0.73425316 0.73425316 0.73425316 0.73425316\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73372492        nan        nan        nan 0.73357406\n",
      " 0.7341023  0.73417772 0.7341023  0.7341023  0.7341023  0.7341023\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364947        nan        nan        nan 0.73349864\n",
      " 0.73402672 0.73425305 0.73402672 0.73410216 0.73402675 0.73402672\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.734781405235396\n",
      "best params{'C': 0.026366508987303583, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "best estimator 0.7200612557427258\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_2_norm, y_train_2)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param, cv=5)\n",
    "log_reg2_norm_cv = grid_search.fit(X_train_2_norm, y_train_2)\n",
    "\n",
    "print(f\"best score {log_reg2_norm_cv.best_score_}\")\n",
    "print(f\"best params{log_reg2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {log_reg2_norm_cv.best_estimator_.score(X_val_2_norm, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have now tested Logistic regression. It had a reasonable result, however I will now try other models to see if i get a better result. The best performing model was that of the second data set with standard scaler. This had the best estimator of the four"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will no tet KNN to see what kind of results that will predict with the four different scenario's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m hyper_param \u001b[39m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39mNone\u001b[39;00m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbootstrap\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m],\n\u001b[0;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m     13\u001b[0m model_forest \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m---> 14\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(model_forest, hyper_param, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m rand_for_ss_cv \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest score \u001b[39m\u001b[39m{\u001b[39;00mrand_for_ss_cv\u001b[39m.\u001b[39mbest_score_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "hyper_param = {\n",
    "    'n_estimators': [10,20,50, 100],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5, verbose=0)\n",
    "rand_for_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"best score {rand_for_ss_cv.best_score_}\")\n",
    "print(f\"best params{rand_for_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-NSZCLOcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
