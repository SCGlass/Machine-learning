{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will begin to sort the data so that I can test different types of models to see which is best suited to predicting if the patient has a cardiovascular disease or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the saved data frame that I had cleaned and added features to.\n",
    "disease_data = \"..\\data\\Heart_disease.csv\"\n",
    "df_disease = pd.read_csv(disease_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BMI_cat</th>\n",
       "      <th>bp_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.93</td>\n",
       "      <td>obese (class I)</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>48.25</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.71</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>61.83</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>95.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.98</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>63.10</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>90.0</td>\n",
       "      <td>145</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.05</td>\n",
       "      <td>obese (class II)</td>\n",
       "      <td>Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>60.07</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>82.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.40</td>\n",
       "      <td>overweight</td>\n",
       "      <td>Stage 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   1  55.38       1     156    85.0    140     90            3     1      0   \n",
       "1   3  48.25       2     169    82.0    150    100            1     1      0   \n",
       "2  12  61.83       2     178    95.0    130     90            3     3      0   \n",
       "3  32  63.10       1     158    90.0    145     85            2     2      0   \n",
       "4  46  60.07       2     173    82.0    140     90            3     1      0   \n",
       "\n",
       "   alco  active  cardio    BMI           BMI_cat bp_category  \n",
       "0     0       1       1  34.93   obese (class I)     Stage 2  \n",
       "1     0       1       1  28.71        overweight     Stage 2  \n",
       "2     0       1       1  29.98        overweight     Stage 1  \n",
       "3     0       1       1  36.05  obese (class II)     Stage 1  \n",
       "4     0       0       1  27.40        overweight     Stage 2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19788 entries, 0 to 19787\n",
      "Data columns (total 16 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           19788 non-null  int64  \n",
      " 1   age          19788 non-null  float64\n",
      " 2   gender       19788 non-null  int64  \n",
      " 3   height       19788 non-null  int64  \n",
      " 4   weight       19788 non-null  float64\n",
      " 5   ap_hi        19788 non-null  int64  \n",
      " 6   ap_lo        19788 non-null  int64  \n",
      " 7   cholesterol  19788 non-null  int64  \n",
      " 8   gluc         19788 non-null  int64  \n",
      " 9   smoke        19788 non-null  int64  \n",
      " 10  alco         19788 non-null  int64  \n",
      " 11  active       19788 non-null  int64  \n",
      " 12  cardio       19788 non-null  int64  \n",
      " 13  BMI          19788 non-null  float64\n",
      " 14  BMI_cat      19788 non-null  object \n",
      " 15  bp_category  19788 non-null  object \n",
      "dtypes: float64(3), int64(11), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_disease.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>BMI_cat_normal range</th>\n",
       "      <th>BMI_cat_obese (class I)</th>\n",
       "      <th>BMI_cat_obese (class II)</th>\n",
       "      <th>BMI_cat_obese (class III)</th>\n",
       "      <th>BMI_cat_overweight</th>\n",
       "      <th>BMI_cat_underweight</th>\n",
       "      <th>bp_category_Elevated</th>\n",
       "      <th>bp_category_Healthy</th>\n",
       "      <th>bp_category_Stage 1</th>\n",
       "      <th>bp_category_Stage 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  cholesterol  gluc  smoke  alco  active  cardio  female  male  \\\n",
       "0   1  55.38            3     1      0     0       1       1       1     0   \n",
       "\n",
       "   BMI_cat_normal range  BMI_cat_obese (class I)  BMI_cat_obese (class II)  \\\n",
       "0                     0                        1                         0   \n",
       "\n",
       "   BMI_cat_obese (class III)  BMI_cat_overweight  BMI_cat_underweight  \\\n",
       "0                          0                   0                    0   \n",
       "\n",
       "   bp_category_Elevated  bp_category_Healthy  bp_category_Stage 1  \\\n",
       "0                     0                    0                    0   \n",
       "\n",
       "   bp_category_Stage 2  \n",
       "0                    1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the criteria for the first dataframe \n",
    "# I drop all the columns that are not needed\n",
    "df_one = df_disease.drop(columns=[\"ap_hi\", \"ap_lo\", \"height\", \"weight\", \"BMI\"])\n",
    "# Get dummies (one hot encoding) makes columns for categorical data and then represents them with a 1 if they are applicable\n",
    "df_one = pd.get_dummies(df_one, columns=[\"gender\",\"BMI_cat\", \"bp_category\"])\n",
    "# I rename the column for gender so I know that they are either male or female\n",
    "df_one.rename(columns= {\"gender_1\" : \"female\", \"gender_2\" : \"male\"}, inplace=True)\n",
    "df_one.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.38</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  active  cardio  \\\n",
       "0   1  55.38    140     90            3     1      0     0       1       1   \n",
       "\n",
       "     BMI  female  male  \n",
       "0  34.93       1     0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the columns for the criteria of the second data frame\n",
    "df_two = df_disease.drop(columns=[\"BMI_cat\", \"bp_category\", \"height\", \"weight\"])\n",
    "# one hot encoding one the gender column this time\n",
    "df_two = pd.get_dummies(df_two, columns=[\"gender\"])\n",
    "# changing the gender columns to male and female\n",
    "df_two.rename(columns= {\"gender_1\" : \"female\", \"gender_2\" : \"male\"}, inplace=True)\n",
    "df_two.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project I will begin to test some different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing scaling so that it can improve the algorithm by ensures the model will not be biased\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# train test split will split the data so we can train the model then test it.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19788, 19), (19788,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating train test split and validation for the first data frame, Cardio is on its own as this is what we want to predict.\n",
    "X,y = df_one.drop(\"cardio\", axis = \"columns\"), df_one[\"cardio\"]\n",
    "\n",
    "X.shape, y.shape # The shapes of the training data and the removed cardio column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train = (13257, 19)\n",
      " X test = (3266, 19)\n",
      " X val = (3265, 19)\n",
      " y train = (13257,)\n",
      " y test = (3266,)\n",
      " y val = (3265,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting The data frame into train|testsplit, using random_state = 42 to keep the same split over the data set, doing a 70/30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \n",
    "# splitting the test data 50/50 to get validation data to train on. This is good so that we dont leak the test data all the time \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"X train = {X_train.shape}\\n X test = {X_test.shape}\\n X val = {X_val.shape}\\n y train = {y_train.shape}\\n y test = {y_test.shape}\\n y val = {y_val.shape}\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling and normalization\n",
    "\n",
    "# creating a standard scaler object, this subtracts the mean of the feature and dividing it by the standard deviation\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# creating a MinMaxScaler object, This scales to a fixed range, normally 0 to 1. It subtracts the min value from the feature and then divides by the range.\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fitting and transforming the data using standard scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Fit and transforming the data using the minmax scaler\n",
    "X_train_norm = minmax_scaler.fit_transform(X_train)\n",
    "X_val_norm = minmax_scaler.transform(X_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split the second datframe and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train = (13257, 12)\n",
      " X test = (3266, 12)\n",
      " X val = (3265, 12)\n",
      " y train = (13257,)\n",
      " y test = (3266,)\n",
      " y val = (3265,)\n"
     ]
    }
   ],
   "source": [
    "X2,y2 = df_two.drop(\"cardio\", axis = \"columns\"), df_two[\"cardio\"]\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.33, random_state=42) \n",
    "X_val_2, X_test_2, y_val_2, y_test_2 = train_test_split(X_test_2, y_test_2, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# creating a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "# Fitting and transforming the data using standard scaler\n",
    "X_train_2_scaled = scaler.fit_transform(X_train_2)\n",
    "X_val_2_scaled = scaler.transform(X_val_2)\n",
    "# Fit and transforming the data using the minmax scaler\n",
    "X_train_2_norm = minmax_scaler.fit_transform(X_train_2)\n",
    "X_val_2_norm = minmax_scaler.transform(X_val_2)\n",
    "\n",
    "print(f\"X train = {X_train_2.shape}\\n X test = {X_test_2.shape}\\n X val = {X_val_2.shape}\\n y train = {y_train_2.shape}\\n y test = {y_test_2.shape}\\n y val = {y_val_2.shape}\"  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression Data set 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now train and test a logistic regression model on my split and processed data. I have chosen this algorithm as it is good for classifying data into different groups. As we have many features with categorical data such as binary and nominal. It will produce probability scores using a sigmoid function. Then predict whether the outcome is likely on a probability scale from 0 to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.70672051        nan        nan        nan 0.70853072\n",
      " 0.7062683  0.70596656 0.7062683  0.7062683  0.7062683  0.7062683\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70807826        nan        nan        nan 0.70837986\n",
      " 0.70619289 0.70619289 0.70619289 0.70619289 0.70619289 0.70619289\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70815362        nan        nan        nan 0.707173\n",
      " 0.70626827 0.70596659 0.70626827 0.70626827 0.70626827 0.70626827\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70815356        nan        nan        nan 0.70755024\n",
      " 0.70626827 0.70604203 0.70626827 0.70626827 0.70626827 0.70626827\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70724867        nan        nan        nan 0.70694704\n",
      " 0.70657005 0.70611744 0.70657005 0.70657005 0.70657005 0.70657005\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70702242        nan        nan        nan 0.70664523\n",
      " 0.70641919 0.70626833 0.70641919 0.70641919 0.70641919 0.70641919\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70687159        nan        nan        nan 0.70634366\n",
      " 0.70634377 0.7064946  0.70634377 0.70634377 0.70634377 0.70634377\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70687165        nan        nan        nan 0.7061928\n",
      " 0.70649466 0.70664549 0.70649466 0.70649466 0.70649466 0.70649466\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70672082        nan        nan        nan 0.70656993\n",
      " 0.70664552 0.70664552 0.70664552 0.70657008 0.70664552 0.70664552\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70634369        nan        nan        nan 0.70664538\n",
      " 0.70687182 0.70657008 0.70687182 0.70679638 0.70687182 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70604197        nan        nan        nan 0.7064946\n",
      " 0.70687179 0.70672093 0.70687179 0.70687179 0.70687179 0.70687179\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70611739        nan        nan        nan 0.70619283\n",
      " 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70634372        nan        nan        nan 0.70626827\n",
      " 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179 0.70687179\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7064946         nan        nan        nan 0.70641916\n",
      " 0.70679635 0.70679635 0.70679635 0.70679635 0.70679635 0.70679635\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70664546        nan        nan        nan 0.70649463\n",
      " 0.70672093 0.70679635 0.70679635 0.70679635 0.70672093 0.70679635\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70679638        nan        nan        nan 0.70664549\n",
      " 0.70679638 0.70679635 0.70672093 0.70672093 0.70672093 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70664552        nan        nan        nan 0.70664552\n",
      " 0.70679638 0.70664552 0.70679638 0.70672093 0.70679638 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70672096        nan        nan        nan 0.7065701\n",
      " 0.70679638 0.70664552 0.70679638 0.70679638 0.70679638 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70664555        nan        nan        nan 0.70664555\n",
      " 0.70679638 0.70672096 0.70679638 0.70679638 0.70672096 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7065701         nan        nan        nan 0.70664552\n",
      " 0.70679638 0.70672096 0.70679638 0.70679638 0.70679638 0.70679638\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.7085307175128996\n",
      "best params{'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Val accuracy 0.6934150076569678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# defining the hyper parameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    # This controls the coefficients, a small c means stronger regularization and simpler model,larger C means weaker regularization and more complex model \n",
    "    # np logspace searches the values between -2 and 20\n",
    "    \"C\": np.logspace(-2,0,20),  \n",
    "    # This determines the type of regularization to used. L1(Lasso), L2(Ridge)and elastic net is a combination of the two.\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    # This defines which algorithm will be used to optimize the model. \n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "# This is the grid search which will test the hyper parameters that we have chosen\n",
    "# refit will find the best parameters based on accuracy, cv is the amount of cross folds that will be tested and verbose means that progress will not be printed in the console  \n",
    "classifier_log_reg_ss = GridSearchCV(model, hyper_param, refit=\"accuracy\", cv=5, verbose=0)\n",
    "# Fitting the Grid search to the training data\n",
    "log_reg_ss_cv = classifier_log_reg_ss.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Printing out the accuracy of the model and the best parameters. I also printed out the best estimator score based on the validation data.\n",
    "print(f\"Train accuracy {log_reg_ss_cv.best_score_}\")\n",
    "print(f\"best params{log_reg_ss_cv.best_params_}\")\n",
    "print(f\"Val accuracy {log_reg_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min max scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now repeat the process with the Min max scaler on the first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.7072482964110984\n",
      "best params{'C': 0.04281332398719394, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Val accuracy 0.692802450229709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.69638658        nan        nan        nan 0.69834765\n",
      " 0.70310006 0.70264771 0.70310006 0.70310006 0.70310006 0.70310006\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.69797054        nan        nan        nan 0.70566402\n",
      " 0.70415603 0.7037036  0.70415603 0.70415603 0.70415603 0.70415603\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70226964        nan        nan        nan 0.70513614\n",
      " 0.70430689 0.70340186 0.70438234 0.70438234 0.70438234 0.70438234\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70272224        nan        nan        nan 0.70679569\n",
      " 0.70423128 0.70362816 0.70423128 0.70423128 0.70423128 0.70423128\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70377848        nan        nan        nan 0.70664498\n",
      " 0.70408045 0.70430689 0.70408045 0.70408045 0.70408045 0.70408045\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70551339        nan        nan        nan 0.70717291\n",
      " 0.70445778 0.70415598 0.70445778 0.70445778 0.70445778 0.70445778\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7072483         nan        nan        nan 0.70619215\n",
      " 0.70491047 0.70392968 0.70491047 0.70491047 0.70498588 0.70491047\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7058153         nan        nan        nan 0.70702205\n",
      " 0.70521221 0.70453319 0.70528765 0.70528765 0.70528765 0.70528765\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70513626        nan        nan        nan 0.70649406\n",
      " 0.70551387 0.70551393 0.70551387 0.70551387 0.70551387 0.70551387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70498543        nan        nan        nan 0.70679581\n",
      " 0.7055894  0.70536318 0.7055894  0.7055894  0.7055894  0.7055894\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70445769        nan        nan        nan 0.7055137\n",
      " 0.7052878  0.70483517 0.7052878  0.7052878  0.7052878  0.7052878\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70506121        nan        nan        nan 0.70566464\n",
      " 0.70581579 0.70513688 0.70581579 0.70581579 0.70581579 0.70581579\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7052876         nan        nan        nan 0.70566473\n",
      " 0.70611759 0.70581581 0.70604214 0.70604214 0.70604214 0.70604214\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70558946        nan        nan        nan 0.70543851\n",
      " 0.70619297 0.70604212 0.70619297 0.70619297 0.70626842 0.70619297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70528765        nan        nan        nan 0.70551401\n",
      " 0.70626839 0.70641927 0.70626839 0.70626839 0.70626842 0.70626839\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70558943        nan        nan        nan 0.70581573\n",
      " 0.70649472 0.70604206 0.70649472 0.70649472 0.70649472 0.70649472\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70566493        nan        nan        nan 0.70581573\n",
      " 0.70634386 0.70619294 0.70634386 0.70634386 0.70634386 0.70634386\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70589129        nan        nan        nan 0.70619289\n",
      " 0.70619297 0.70626839 0.70619297 0.70619297 0.70626842 0.70619297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70619297        nan        nan        nan 0.70641922\n",
      " 0.70626842 0.70634383 0.70626842 0.70626842 0.70626842 0.70626842\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.70641922        nan        nan        nan 0.7065701\n",
      " 0.70641922 0.70619294 0.70641922 0.70641922 0.70641922 0.70641922\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_norm, y_train)\n",
    "\n",
    "# defining the hyper parameters I wish to search on the model, I have selected the same parameters\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "classifier_log_reg_norm = GridSearchCV(model, hyper_param,refit=\"accuracy\", cv=5)\n",
    "log_reg_norm_cv = classifier_log_reg_norm.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"Train accuracy {log_reg_norm_cv.best_score_}\")\n",
    "print(f\"best params{log_reg_norm_cv.best_params_}\")\n",
    "print(f\"Val accuracy {log_reg_norm_cv.best_estimator_.score(X_val_norm, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now do the same process on the second dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7347056490820221\n",
      "best params{'C': 0.026366508987303583, 'penalty': 'l1', 'solver': 'saga'}\n",
      "best estimator 0.7231240428790199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.73259378        nan        nan        nan 0.73463035\n",
      " 0.73402697 0.73432858 0.73402697 0.73402697 0.73410239 0.73402697\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349876        nan        nan        nan 0.73395142\n",
      " 0.73395139 0.73410227 0.73395139 0.73387597 0.73387597 0.73395139\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73395139        nan        nan        nan 0.7338005\n",
      " 0.73395127 0.73372509 0.73395127 0.73395127 0.73395127 0.73395127\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342343        nan        nan        nan 0.73387586\n",
      " 0.7338758  0.7338758  0.7338758  0.7338758  0.73395125 0.7338758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334793        nan        nan        nan 0.73470565\n",
      " 0.73364947 0.73380033 0.73364947 0.73357403 0.73357403 0.73364947\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312174        nan        nan        nan 0.73470548\n",
      " 0.7336495  0.7336495  0.7336495  0.7336495  0.73372494 0.7336495\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73387597        nan        nan        nan 0.73387575\n",
      " 0.73357409 0.73357411 0.73357409 0.73357409 0.73357409 0.73357409\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364964        nan        nan        nan 0.73402669\n",
      " 0.73372497 0.73380042 0.73372497 0.73372497 0.73372497 0.73372497\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342328        nan        nan        nan 0.73327231\n",
      " 0.73387583 0.73357409 0.73387583 0.73387583 0.73380042 0.73387583\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357406        nan        nan        nan 0.73327228\n",
      " 0.73357409 0.73342326 0.73357409 0.73357409 0.73364953 0.73357409\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73319687        nan        nan        nan 0.73327231\n",
      " 0.73334787 0.7334987  0.73334787 0.73334787 0.73334787 0.73334787\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312148        nan        nan        nan 0.73304601\n",
      " 0.73327246 0.73327246 0.73327246 0.73327246 0.73327246 0.73327246\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73312146        nan        nan        nan 0.73327237\n",
      " 0.73327248 0.73319701 0.73327248 0.73327248 0.73319704 0.73327248\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327234        nan        nan        nan 0.73334781\n",
      " 0.73319704 0.73312157 0.73319704 0.73319704 0.73319704 0.73319704\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73334781        nan        nan        nan 0.7334987\n",
      " 0.73304615 0.73312157 0.7331216  0.7331216  0.73304615 0.73304615\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327237        nan        nan        nan 0.73357411\n",
      " 0.73312157 0.73312157 0.73312157 0.73312157 0.7331216  0.73312157\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73342323        nan        nan        nan 0.73349867\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349867        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327243        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327243        nan        nan        nan 0.73327243\n",
      " 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701 0.73319701\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the model and training it\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param,refit=\"accuracy\", cv=5)\n",
    "log_reg2_ss_cv = grid_search.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "print(f\"best score {log_reg2_ss_cv.best_score_}\")\n",
    "print(f\"best params{log_reg2_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {log_reg2_ss_cv.best_estimator_.score(X_val_2_scaled, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1600 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'none' (deprecated), 'l1'} or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Sam Glass ITHS\\.virtualenvs\\Machine-learning-NSZCLOcg\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.72324027        nan        nan        nan 0.73274467\n",
      " 0.72557841 0.71743197 0.72557841 0.72557841 0.72557841 0.72550297\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73131159        nan        nan        nan 0.7331218\n",
      " 0.72746427 0.71901622 0.72746427 0.72746427 0.72746427 0.72753971\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73349898        nan        nan        nan 0.73387623\n",
      " 0.72957645 0.72263724 0.72957645 0.72957645 0.72957645 0.72957645\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357446        nan        nan        nan 0.73395139\n",
      " 0.73025535 0.72557895 0.7303308  0.7303308  0.7303308  0.7303308\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73478141        nan        nan        nan 0.73432855\n",
      " 0.73025541 0.72731378 0.73025541 0.73025541 0.73025541 0.73025541\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73432863        nan        nan        nan 0.73349867\n",
      " 0.73146224 0.72799248 0.73146224 0.73146224 0.73153769 0.73146224\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7331971         nan        nan        nan 0.7333479\n",
      " 0.73221662 0.72995369 0.73221662 0.73221662 0.73214118 0.73221662\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73266905        nan        nan        nan 0.73349878\n",
      " 0.73266922 0.73221676 0.73266922 0.73266922 0.73259381 0.73259381\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73259352        nan        nan        nan 0.73380053\n",
      " 0.73236725 0.73236745 0.73236725 0.73236725 0.73229184 0.73251814\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327226        nan        nan        nan 0.73395125\n",
      " 0.73312174 0.73266908 0.73312174 0.73312174 0.73312174 0.73312174\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73387569        nan        nan        nan 0.73447929\n",
      " 0.73447966 0.73342346 0.73447966 0.73447966 0.73440422 0.73440422\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73372483        nan        nan        nan 0.73410219\n",
      " 0.73425342 0.73349904 0.73425342 0.73425342 0.73432886 0.73417797\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73425288        nan        nan        nan 0.7339513\n",
      " 0.73417806 0.73372537 0.73417806 0.73417806 0.73440433 0.73417806\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73357403        nan        nan        nan 0.7336495\n",
      " 0.73380087 0.73327285 0.73387631 0.73387631 0.7338009  0.73387631\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364953        nan        nan        nan 0.73342326\n",
      " 0.73342368 0.73357451 0.73342368 0.73342368 0.73342368 0.73342368\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.7338003         nan        nan        nan 0.73380036\n",
      " 0.73395159 0.73342348 0.73395159 0.73395159 0.73395159 0.73395159\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364944        nan        nan        nan 0.73364944\n",
      " 0.73387609 0.73425325 0.73380064 0.73380064 0.73387609 0.73380064\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73327226        nan        nan        nan 0.73349859\n",
      " 0.73425316 0.7343286  0.73425316 0.73425316 0.73417772 0.73425316\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73372492        nan        nan        nan 0.73357406\n",
      " 0.7341023  0.73417772 0.7341023  0.7341023  0.7341023  0.7341023\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.73364947        nan        nan        nan 0.7336495\n",
      " 0.73402672 0.73425305 0.73402672 0.73410216 0.7341776  0.73402672\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.734781405235396\n",
      "best params{'C': 0.026366508987303583, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "best estimator 0.7200612557427258\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_2_norm, y_train_2)\n",
    "\n",
    "# defining the hyperparameters I wish to test on the model\n",
    "hyper_param = {\n",
    "    \"C\": np.logspace(-2,0,20),\n",
    "    \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"None\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "\n",
    "grid_search = GridSearchCV(model, hyper_param,refit=\"accuracy\", cv=5)\n",
    "log_reg2_norm_cv = grid_search.fit(X_train_2_norm, y_train_2)\n",
    "\n",
    "print(f\"best score {log_reg2_norm_cv.best_score_}\")\n",
    "print(f\"best params{log_reg2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {log_reg2_norm_cv.best_estimator_.score(X_val_2_norm, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have now tested Logistic regression. It had a reasonable result, however I will now try other models to see if i get a better result. The best performing model was that of the second data set with standard scaler. This had the best estimator of the four"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now test Random Forest to see what kind of results that will predict with the four different scenario's. Random forest classifier is an algorithm thats classifies data by combining multiple decision trees. Each decision tree is trained on a random It helps in reducing over fitting as each tree is less likely to be biased. It takes the majority vote of all the individual decision trees to make a final classification. This makes it more accurate and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6736828869608953\n",
      "best params{'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 150}\n",
      "best estimator 0.6643185298621745\n"
     ]
    }
   ],
   "source": [
    "# Importing random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "hyper_param = {\n",
    "    # This determines the number of of decision trees to be used.More trees means more accuracy, however it can be slower to train and cause overfitting\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    # The amount of features to be considered when splitting a node, sqrt does this by square root and log2 on logarithm.\n",
    "    \"max_features\": ['sqrt', 'log2'],\n",
    "    # This evaluated the quality of each split in the decision tree, gini measures the nodes impurity by the incorrect probability\n",
    "    # entropy measures the amount of info gained splitting the a node based on the distribution class\n",
    "    \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "# chosing the model\n",
    "model_forest = RandomForestClassifier()\n",
    "# implementing grid search , entering in hyper parameters and crossfold at 5\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5,refit=\"accuracy\", verbose=0)\n",
    "# fitiing gridsearch to the data\n",
    "rand_for_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"best score {rand_for_ss_cv.best_score_}\")\n",
    "print(f\"best params{rand_for_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6736077566563288\n",
      "best params{'criterion': 'gini', 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "best estimator 0.6621745788667688\n"
     ]
    }
   ],
   "source": [
    "hyper_param = {\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    \"max_features\": ['sqrt', 'log2'],\n",
    "    \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5,refit=\"accuracy\", verbose=0)\n",
    "rand_for_norm_cv = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"best score {rand_for_norm_cv.best_score_}\")\n",
    "print(f\"best params{rand_for_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for_norm_cv.best_estimator_.score(X_val_norm, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7276921028690617\n",
      "best params{'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 150}\n",
      "best estimator 0.7261868300153139\n"
     ]
    }
   ],
   "source": [
    "hyper_param = {\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    \"max_features\": ['sqrt', 'log2'],\n",
    "    \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5,refit=\"accuracy\", verbose=0)\n",
    "rand_for2_ss_cv = grid_search.fit(X_train_2_scaled, y_train_2)\n",
    "\n",
    "print(f\"best score {rand_for2_ss_cv.best_score_}\")\n",
    "print(f\"best params{rand_for2_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for2_ss_cv.best_estimator_.score(X_val_2_scaled, y_val_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min max scaler dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7271638864755781\n",
      "best params{'criterion': 'entropy', 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "best estimator 0.7283307810107198\n"
     ]
    }
   ],
   "source": [
    "hyper_param = {\n",
    "    \"n_estimators\": [100, 150, 200],\n",
    "    \"max_features\": ['sqrt', 'log2'],\n",
    "    \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model_forest, hyper_param, cv=5,refit=\"accuracy\", verbose=0)\n",
    "rand_for2_norm_cv = grid_search.fit(X_train_2_norm, y_train)\n",
    "\n",
    "print(f\"best score {rand_for2_norm_cv.best_score_}\")\n",
    "print(f\"best params{rand_for2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {rand_for2_norm_cv.best_estimator_.score(X_val_2_norm, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli naive bayes model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my 3rd model I have chosen to try a Bernoulli naive bayes algorithm. It uses probability distributions to represent different possible outcomes. It will calculate the probability of each possible outcome based on observed features, then choose the outcome with the highest probability. It works very well for binary features, but can be inaccurate if there is high correlations between features and if they are not binary. Within the dataset there is a good portion of binary features. So I thought it would be a good model to try"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set one standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6997051967640203\n",
      "best params{'alpha': 0.001, 'binarize': 0.2, 'fit_prior': True}\n",
      "best estimator 0.6909647779479327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    # alpha controls the smoothing of the probabilities for each feature. a larger alpha causes more generalization and a smaller could cause over fitting\n",
    "    \"alpha\": np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    #  This contols whether to learn class prior probabilities from the data\n",
    "    \"fit_prior\": [True, False],\n",
    "    # This converts features that are not binary to binary \n",
    "    \"binarize\": np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid,refit=\"accuracy\", cv=5)\n",
    "mnb_ss_cv = grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"best score {mnb_ss_cv.best_score_}\")\n",
    "print(f\"best params{mnb_ss_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_ss_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max min scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.6991782036204784\n",
      "best params{'alpha': 1.0, 'binarize': 0.7000000000000001, 'fit_prior': True}\n",
      "best estimator 0.6900459418070444\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    \"fit_prior\": [True, False],\n",
    "    \"binarize\": np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid,refit=\"accuracy\", cv=5)\n",
    "mnb_norm_cv = grid_search.fit(X_train_norm, y_train)\n",
    "\n",
    "print(f\"best score {mnb_norm_cv.best_score_}\")\n",
    "print(f\"best params{mnb_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_norm_cv.best_estimator_.score(X_val_scaled, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set 2 Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7232408385691276\n",
      "best params{'alpha': 10.0, 'binarize': 0.0, 'fit_prior': True}\n",
      "best estimator 0.7075038284839203\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    \"fit_prior\": [True, False],\n",
    "    \"binarize\": np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid,refit=\"accuracy\", cv=5)\n",
    "mnb_ss2_cv = grid_search.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "print(f\"best score {mnb_ss2_cv.best_score_}\")\n",
    "print(f\"best params{mnb_ss2_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_ss2_cv.best_estimator_.score(X_val_2_scaled, y_val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min max scaler dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7239194578101096\n",
      "best params{'alpha': 0.001, 'binarize': 0.5, 'fit_prior': True}\n",
      "best estimator 0.691271056661562\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 7), # tests values from 0.001 to 1000\n",
    "    \"fit_prior\": [True, False],\n",
    "    \"binarize\": np.linspace(0.0, 1.0, 11)} # will test 11 values between 0.0 to 1.0\n",
    "\n",
    "model_mnb = BernoulliNB()\n",
    "grid_search = GridSearchCV(model_mnb, param_grid,refit=\"accuracy\", cv=5)\n",
    "mnb_ss2_norm_cv = grid_search.fit(X_train_2_norm, y_train)\n",
    "\n",
    "print(f\"best score {mnb_ss2_norm_cv.best_score_}\")\n",
    "print(f\"best params{mnb_ss2_norm_cv.best_params_}\")\n",
    "print(f\"best estimator {mnb_ss2_norm_cv.best_estimator_.score(X_val_2_scaled, y_val)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-NSZCLOcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
